{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e72a842",
   "metadata": {},
   "source": [
    "# Prepares data to go into SHETRAN and HIPIMS\n",
    "\n",
    "## License\n",
    "Python script implementing algorithm for merging and gridding rainfall data  \n",
    "Copyright (C) 2023  Amy Green & Newcastle University  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU General Public License as published by  \n",
    "the Free Software Foundation, either version 3 of the License, or  \n",
    "(at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the  \n",
    "GNU General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU General Public License  \n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "## Notes\n",
    "- This code relies on the following code\n",
    "    1. `read_met` - reads in 5-minute gridded composite and processed C-band weather radar from the Met Office ftp server.\n",
    "    2. `read_ea` - reads in 15-minute rain gauge data from the EA website API.\n",
    "    3. `read_cs` - reads in varying resolution rain gauge data from NGIF and UO APIand accumulates up to 15-minute resolution data.\n",
    "    4. `read_cs_radar` - reads in Newcastle University Urban Observatory X-band radar code, processes it and accumulates it up to 15 minutes.\n",
    "    5. `intense_qc` - quality controls all gauge data using intense-qc code (Note: this is designed for hourly rain gauge data). \n",
    "- It also requires the bounding box shapefile for HIPIMS and the Tyne mask file for SHETRAN (these are in a `static.zip` file).\n",
    "\n",
    "## What does this code do?\n",
    "\n",
    "### Reads in data\n",
    "1. Read in radar data (if exists)\n",
    "    - Met Office radar data\n",
    "    - Urban Observatory radar data\n",
    "2. Read in quality controlled rain gauge data (if exists)\n",
    "    - Environment Agency rain gauges\n",
    "    - Urban Observatory rain gauges\n",
    "    - Citizen Science rain gauges\n",
    "    - National Green Infrastructure Facility rain gauges\n",
    "3. Identifies best data to use\n",
    "    - Uses data priority for HIPIMS\n",
    "    - Uses data priority for SHETRAN\n",
    "4. Grids/merges data\n",
    "    - Combines different data types to provide gridded 15-minute rainfall estimates (1km resolution)\n",
    "4. Saves data in correct format for models\n",
    "    - Clips and saves data in correct format for HIPIMS\n",
    "    - Clips and saves data in correct format for SHETRAN\n",
    "    \n",
    "### Prioritises data for input into HIPIMS model\n",
    "1. High resolution (250m) X-band radar data, merged with rain gauges, which is conditional on Met Office C-band radar data.\n",
    "2. High resolution (250m) X-band radar data, conditional on Met Office C-band radar data.\n",
    "3. High resolution (250m) X-band radar data, merged with rain gauges.\n",
    "4. Merged Met Office radar data and quality controlled rain gauge data.\n",
    "5. Met Office radar data.\n",
    "6. Gridded quality controlled rain gauge data (IF Environment Agency rain gauge data exists).\n",
    "\n",
    "NOTE: 1. and 2. are missing as cannot test this whilst the Urban Observatory radar is down for maintenence and API is not working.\n",
    "\n",
    "### Prioritises data for input into SHETRAN model\n",
    "1. Merged Met Office radar data and quality controlled rain gauge data.\n",
    "2. Met Office radar data.\n",
    "3. Gridded quality controlled rain gauge data (IF Environment Agency rain gauge data exists). \n",
    "\n",
    "\n",
    "## File structure\n",
    "\n",
    "### Data inputs format \n",
    "- `\\inputs` - root inputs data folder\n",
    "\n",
    "### Data outputs format\n",
    "- `\\outputs` - root outputs data folder\n",
    "    - `\\SHETRAN` - SHETRAN rainfall input data, which should be in the correct format to run the model if running at 15 minute resolution (Note: the start date and data resolution should be added into the library SHETRAN set-up file).\n",
    "    - `\\HIPIMS` - HIPIMS rainfall input data, which should be in the correct format to run the model at 15 minute resolution\n",
    "\n",
    "## Data minimum requirements\n",
    "\n",
    "### Minimum data requirement to run model\n",
    "Either Met Office C-band radar data, or Environment Agency rain gauge data.\n",
    "\n",
    "\n",
    "- SHETRAN 1 and 2\n",
    "    1. Requirements:\n",
    "        - `inputs\\MET` exists and contains the files `arrays.npy`, `timestamp.csv`, `coords_x.csv` and `coords_y.csv`\n",
    "    2. Additional information if available:\n",
    "        - `inputs\\EA\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\UO\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\CS\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\NGIF\\15min` exists and contains any `.csv` files\n",
    "\n",
    "\n",
    "- SHETRAN 3\n",
    "    1. Requirements:\n",
    "        - `inputs\\EA\\15min` exists and contains more than one `.csv` file\n",
    "    2. Additional information if available:\n",
    "        - `inputs\\UO\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\CS\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\NGIF\\15min` exists and contains any `.csv` files\n",
    "\n",
    "\n",
    "- HIPIMS 1\n",
    "    1. Requirements:\n",
    "        - `inputs\\UO\\radar\\15min` exists and contains the files `arrays.npy`, `timestamp.csv`, `coords_x.csv` and `coords_y.csv`\n",
    "    - `inputs\\MET` exists and contains the files `arrays.npy`, `timestamp.csv`, `coords_x.csv` and `coords_y.csv`\n",
    "    2. Additional information if available:\n",
    "        - `inputs\\EA\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\UO\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\CS\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\NGIF\\15min` exists and contains any `.csv` files\n",
    "\n",
    "\n",
    "- HIPIMS 2\n",
    "    1. Requirements:\n",
    "        - `inputs\\UO\\radar\\15min` exists and contains the files `arrays.npy`, `timestamp.csv`, `coords_x.csv` and `coords_y.csv`\n",
    "        - `inputs\\MET` exists and contains the files `arrays.npy`, `timestamp.csv`, `coords_x.csv` and `coords_y.csv`\n",
    "    2. Additional information if available:\n",
    "        - `inputs\\EA\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\UO\\15min` exists and contains any `.csv` files\n",
    "        - `input\\CS\\15min` exists and contains any `.csv` files\n",
    "        - `inputs\\NGIF\\15min` exists and contains any `.csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Prepare Data or DAFNI workflow\n",
    "# Amy Green, Robin Wardle\n",
    "# July 2023\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# Python libraries\n",
    "###############################################################################\n",
    "from os.path import join, exists, split\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import wradlib.ipol as ipol\n",
    "import gstools as gs\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Constants and Parameter Handling\n",
    "###############################################################################\n",
    "PREPARE_DATA_SUCCESS_FILENAME = \"success\"\n",
    "PREPARE_DATA_LOG_FILENAME = \"prepare-data.log\"\n",
    "\n",
    "def validate_boolean_parameter(p_string):\n",
    "    # Boolean parameters are passed in from DAFNI as \"True\" and \"False\"\n",
    "    # but to avoid any pointless errors e.g. maybe this might change,\n",
    "    # it is a good idea to lowercase the parameter strings.\n",
    "    p_val = str.lower(os.getenv(p_string, \"false\"))\n",
    "    if p_val == \"true\":\n",
    "      p_val = True\n",
    "    elif p_val == \"false\":\n",
    "        p_val = False\n",
    "    else:\n",
    "        e = \"Model run terminated: undefined value provided for parameter {} --- {}\".format(p_string, p_val)\n",
    "        raise ValueError(e)\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Fundamental Paths\n",
    "###############################################################################\n",
    "\n",
    "# Input and output paths\n",
    "platform = os.getenv(\"PREPARE_DATA_ENV\")\n",
    "if platform==\"docker\":\n",
    "    data_path = os.getenv(\"DATA_PATH\", \"/data\")\n",
    "else:\n",
    "    data_path = os.getenv(\"DATA_PATH\", \"./data\")\n",
    "\n",
    "input_path = os.path.join(data_path, \"inputs\")\n",
    "output_path = os.path.join(data_path, \"outputs\")\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Reset the success and log files\n",
    "if os.path.isfile(os.path.join(output_path, PREPARE_DATA_SUCCESS_FILENAME)):\n",
    "    os.remove(os.path.join(output_path, PREPARE_DATA_SUCCESS_FILENAME))\n",
    "if os.path.isfile(os.path.join(output_path, PREPARE_DATA_LOG_FILENAME)):\n",
    "    os.remove(os.path.join(output_path, PREPARE_DATA_LOG_FILENAME))\n",
    "    \n",
    "\n",
    "# Simulator-specific output paths\n",
    "output_shetran_path = join(output_path, \"SHETRAN\")\n",
    "if not exists(output_shetran_path):\n",
    "    os.mkdir(output_shetran_path)\n",
    "\n",
    "output_hipims_path = join(output_path, \"HIPIMS\")\n",
    "if not exists(output_hipims_path):\n",
    "    os.mkdir(output_hipims_path)\n",
    "\n",
    "# path to files needed to clip data (e.g. bounding box for HIPIMS, mask file for SHETRAN)\n",
    "static_path = join(input_path, \"static\")\n",
    "\n",
    "# Check whether a pre-prepared dataset should be used\n",
    "if os.getenv(\"READ_MODE\") == \"dataset\":\n",
    "    input_path = os.path.join(input_path, \"preprepared\")\n",
    "\n",
    "# set up a function that reads the header lines from a SHETRAN mask:\n",
    "def read_ascii_header(file_path: str):\n",
    "\n",
    "    header_dict = {}\n",
    "    line=[]\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            line = line.split()\n",
    "            if len(line)>2: break\n",
    "            header_dict[line[0]] = float(line[1])\n",
    "\n",
    "    return header_dict\n",
    "    \n",
    "# path to SHETRAN mask (needs changing)\n",
    "shetran_mask_path = join(static_path, \"Tyne_at_Newcastle_Mask.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e716652",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Logging\n",
    "###############################################################################\n",
    "# Configure logging\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Logging instance\n",
    "#logger = logging.getLogger(pathlib.PurePath(__file__).name)\n",
    "logger = logging.getLogger(\"prepare-data\")\n",
    "logger.propagate = False\n",
    "\n",
    "# Console messaging\n",
    "console_formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "console_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(console_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File logging\n",
    "file_formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "file_handler = logging.FileHandler(output_path / pathlib.Path(PREPARE_DATA_LOG_FILENAME))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Logger initialised\")\n",
    "\n",
    "# Some additional logging info\n",
    "logger.info(\"data_path = {}\".format(data_path))\n",
    "logger.info(\"output_path = {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Other Parameters\n",
    "#   Remember that parameters are passed as environment variables,\n",
    "#   i.e. they are strings and will need to be converted\n",
    "###############################################################################\n",
    "\n",
    "# Dates for files\n",
    "start_date = os.getenv(\"RUN_START_DATE\", \"2023-06-20\")\n",
    "end_date = os.getenv(\"RUN_END_DATE\", \"2023-06-30\")\n",
    "\n",
    "# Bounding box for data\n",
    "# e_l, n_l, e_u, n_u = [355000, 534000, 440000, 609000]\n",
    "try:\n",
    "    e_l = int(os.getenv(\"BB_E_L\", \"355000\"))\n",
    "    n_l = int(os.getenv(\"BB_N_L\", \"534000\"))\n",
    "    e_u = int(os.getenv(\"BB_E_U\", \"440000\"))\n",
    "    n_u = int(os.getenv(\"BB_N_U\", \"609000\"))\n",
    "    bbox = [e_l, e_u, n_l, n_u]\n",
    "    res = int(os.getenv(\"RESOLUTION\", \"1000\"))\n",
    "except (TypeError, ValueError, Exception) as e:\n",
    "    logger.error(\"Error converting environmental parameters: {}\".format(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3684967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for reading in rain gauge data\n",
    "def read_gauges(source):\n",
    "    \n",
    "    gauges = []\n",
    "    \n",
    "    root_path = join(input_path, source)\n",
    "    folder_path = join(root_path, \"15min\")\n",
    "    \n",
    "    if exists(folder_path):\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.endswith(\".csv\"):\n",
    "                data = pd.read_csv(join(folder_path, f), index_col=0)\n",
    "                data.index = pd.to_datetime(data.index, utc=True)\n",
    "                data.columns = [source + \"_\" + f.split(\".\")[0]]\n",
    "                gauges.append(data)\n",
    "                \n",
    "    return gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "met_path = join(join(input_path, \"MET\"), \"15min\")\n",
    "uo_path = join(input_path, \"UO\")\n",
    "uo_radar_path = join(join(uo_path, \"radar\"), \"15min\")\n",
    "\n",
    "shetran_method = np.nan\n",
    "hipims_method = np.nan\n",
    "\n",
    "radar_files = ['arrays.npy', 'coords_x.csv', 'coords_y.csv', 'timestamp.csv']\n",
    "if exists(met_path):\n",
    "    if all([f in os.listdir(met_path) for f in radar_files]):\n",
    "        \n",
    "        # try and read in Met Office weather radar data \n",
    "        met_radar = {}\n",
    "        for f in radar_files:\n",
    "            if f.endswith(\".csv\"):\n",
    "                data = pd.read_csv(join(met_path, f)).iloc[:, 0]\n",
    "            else:\n",
    "                data = np.load(join(met_path, f))\n",
    "\n",
    "            met_radar[f.split(\".\")[0]] = data\n",
    "        \n",
    "        # try and read in rain gauge data\n",
    "        gauges = []\n",
    "        for source in [\"EA\", \"CS\", \"UO\", \"NGIF\"]:\n",
    "            gauges.extend(read_gauges(source))\n",
    "\n",
    "        if len(gauges) > 0:\n",
    "            gauges = pd.concat(gauges, 1)\n",
    "            # SHETRAN INPUT 1: merge radar and gauges together\n",
    "            shetran_method = 1\n",
    "        else:\n",
    "            # SHETRAN INPUT 2: just radar data\n",
    "            shetran_method = 2\n",
    "else:\n",
    "    \n",
    "    gauges = []\n",
    "    for source in [\"EA\", \"CS\", \"UO\", \"NGIF\"]:\n",
    "        gauges.extend(read_gauges(source))\n",
    "    if len(gauges) > 0:\n",
    "        gauges = pd.concat(gauges, 1)\n",
    "        \n",
    "        # check Environment Agency gauge data available\n",
    "        if sum([col.split(\"_\")[0] == \"EA\" for col in gauges.columns]) > 1: \n",
    "            \n",
    "            # SHETRAN INPUT 3: gridded rain gauge data\n",
    "            shetran_method = 3\n",
    "\n",
    "if exists(uo_radar_path):\n",
    "    \n",
    "    if all([f in os.listdir(uo_radar_path) for f in radar_files]):\n",
    "        \n",
    "        # try and read in Urban Observatory weather radar data \n",
    "        uo_radar = {}\n",
    "        for f in radar_files:\n",
    "            if f.endswith(\".csv\"):\n",
    "                data = pd.read_csv(join(uo_radar_path, f)).iloc[:, 0]\n",
    "            else:\n",
    "                data = np.load(join(uo_radar_path, f))\n",
    "\n",
    "            uo_radar[f.split(\".\")[0]] = data\n",
    "            \n",
    "        if uo_radar[\"arrays\"].shape[0] > 0:\n",
    "            \n",
    "            hipims_method = shetran_method\n",
    "            \n",
    "            # HIPIMS INPUT 1, 2 or 3: High-resolution X-band radar data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.isnan(hipims_method):\n",
    "    \n",
    "    if hipims_method == 1:\n",
    "        \n",
    "        # HIPIMS INPUT 1: High-resolution X-band radar data, merged with rain gauges, which is conditional on Met Office C-band radar data.\n",
    "        \n",
    "        \"\"\"\n",
    "        # NOTE: Code not written as high-resolution X-band radar data \n",
    "                was down for maintenence/broken for the duration of \n",
    "                the PYRAMID project, but this is where the code would\n",
    "                go for high resolution (250m) X-band radar data, merged \n",
    "                with rain gauges, which is conditional on Met Office \n",
    "                C-band radar data.\n",
    "                \n",
    "                If code is included, output should be a dictionary \n",
    "                named high_res_rainfall, with attributes mirroring \n",
    "                full_rainfall.\n",
    "        \"\"\"\n",
    "        \n",
    "    elif hipims_method == 2:\n",
    "        \n",
    "        # HIPIMS INPUT 2: High-resolution X-band radar data, conditional on Met Office C-band radar data\n",
    "        \n",
    "        \"\"\"\n",
    "        # NOTE: Code not written as high-resolution X-band radar data \n",
    "                was down for maintenence/broken for the duration of \n",
    "                the PYRAMID project, but this is where the code would\n",
    "                go for high resolution (250m) X-band radar data, \n",
    "                conditional on Met Office C-band radar data.\n",
    "                                \n",
    "                If code is included, output should be a dictionary \n",
    "                named high_res_rainfall, with attributes mirroring \n",
    "                full_rainfall.\n",
    "        \"\"\"\n",
    "        \n",
    "    elif hipims_method == 3:\n",
    "        \n",
    "        # HIPIMS INPUT 3: High-resolution X-band radar data, merged with rain gauges.\n",
    "        \n",
    "        \"\"\"\n",
    "        # NOTE: Code not written as high-resolution X-band radar data \n",
    "                was down for maintenence/broken for the duration of \n",
    "                the PYRAMID project, but this is where the code would\n",
    "                go for high resolution (250m) X-band radar data, \n",
    "                merged with rain gauges.\n",
    "                                \n",
    "                If code is included, output should be a dictionary \n",
    "                named high_res_rainfall, with attributes mirroring \n",
    "                full_rainfall.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ab3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new coordinates for SHETRAN\n",
    "mask = np.loadtxt(shetran_mask_path, skiprows=6)\n",
    "\n",
    "# get mask meta info\n",
    "meta = read_ascii_header(shetran_mask_path)\n",
    "\n",
    "new_x = np.arange(meta[\"xllcorner\"] + meta[\"cellsize\"] / 2, \n",
    "                  meta[\"xllcorner\"] + meta[\"ncols\"] * meta[\"cellsize\"],\n",
    "                  meta[\"cellsize\"])\n",
    "\n",
    "new_y = np.arange(meta[\"yllcorner\"] + meta[\"cellsize\"] / 2, \n",
    "                  meta[\"yllcorner\"] + meta[\"nrows\"] * meta[\"cellsize\"],\n",
    "                  meta[\"cellsize\"])\n",
    "\n",
    "if not np.isnan(shetran_method):\n",
    "    if shetran_method == 1:\n",
    "\n",
    "        # SHETRAN INPUT 1: Merged Met Office radar data and quality controlled rain gauge data.\n",
    "        full_rainfall = {}\n",
    "        for attr in [\"timestamp\", \"coords_x\", \"coords_y\"]:\n",
    "            full_rainfall[attr] = met_radar[attr]\n",
    "\n",
    "        full_rainfall[\"arrays\"] = np.full(met_radar[\"arrays\"].shape, np.nan)\n",
    "\n",
    "        g_x = np.array([col.split(\"_\")[-2] for col in gauges.columns]).astype(float)\n",
    "        g_y = np.array([col.split(\"_\")[-1] for col in gauges.columns]).astype(float)\n",
    "\n",
    "        # source coordinates\n",
    "        src = np.vstack([g_x, g_y]).transpose()\n",
    "\n",
    "        # target coordinates\n",
    "        xtrg = full_rainfall[\"coords_x\"]\n",
    "        ytrg = np.flip(full_rainfall[\"coords_y\"]) # check this is correct way round\n",
    "\n",
    "        trg = np.meshgrid(xtrg, ytrg)\n",
    "        trg = np.vstack((trg[0].ravel(), trg[1].ravel())).T\n",
    "\n",
    "        for t in range(met_radar[\"arrays\"].shape[0]):\n",
    "            \n",
    "            rad = met_radar[\"arrays\"][t]\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                gau = np.array(gauges.iloc[t, :])\n",
    "\n",
    "                rad_x = [np.argmin(abs(x - met_radar[\"coords_x\"])) for x in g_x]\n",
    "                rad_y = [np.argmin(abs(y - np.flip(met_radar[\"coords_y\"]))) for y in g_y]\n",
    "\n",
    "                rad_at_g = np.array(rad[rad_y, rad_x])\n",
    "\n",
    "                try:\n",
    "                    # interpolation objects (Kriging with External Drift)\n",
    "                    if not np.isnan(np.nansum(rad_at_g)):\n",
    "                        ked = ipol.ExternalDriftKriging(src, trg, src_drift=rad_at_g, trg_drift=rad.flatten()) \n",
    "                        # should investigate further variogram structure for data\n",
    "                        # default set to cov='1.0 Exp(10000.)'\n",
    "                    else:\n",
    "                        ked = ipol.ExternalDriftKriging(src, trg)\n",
    "\n",
    "                    full_rainfall[\"arrays\"][t] = ked(gau).reshape(rad.shape) \n",
    "                \n",
    "                except:\n",
    "                    full_rainfall[\"arrays\"][t] = rad\n",
    "            \n",
    "            except:\n",
    "                full_rainfall[\"arrays\"][t] = rad\n",
    "                    \n",
    "    elif shetran_method == 2:\n",
    "\n",
    "        # SHETRAN INPUT 2: Met Office radar data.\n",
    "        full_rainfall = met_radar\n",
    "\n",
    "    elif shetran_method == 3:\n",
    "\n",
    "        # SHETRAN INPUT 3: Gridded quality controlled rain gauge data (IF Environment Agency rain gauge data exists). \n",
    "        full_rainfall = {}\n",
    "        full_rainfall[\"timestamp\"] = gauges.index\n",
    "\n",
    "        g_x = np.array([col.split(\"_\")[-2] for col in gauges.columns]).astype(float)\n",
    "        g_y = np.array([col.split(\"_\")[-1] for col in gauges.columns]).astype(float)\n",
    "\n",
    "        full_rainfall[\"coords_x\"] = pd.Series(new_x)\n",
    "        full_rainfall[\"coords_y\"] = pd.Series(new_y)\n",
    "\n",
    "        # source coordinates\n",
    "        src = np.vstack([g_x, g_y]).transpose()\n",
    "\n",
    "        # target coordinates\n",
    "        xtrg = full_rainfall[\"coords_x\"]\n",
    "        ytrg = full_rainfall[\"coords_y\"]\n",
    "        trg = np.meshgrid(xtrg, ytrg)\n",
    "        trg = np.vstack((trg[0].ravel(), trg[1].ravel())).T\n",
    "\n",
    "        # interpolation objects (Ordinary Kriging)\n",
    "        ok = ipol.OrdinaryKriging(src, trg, remove_missing=True)\n",
    "        ok1 = ipol.OrdinaryKriging(src[0:-1, :], trg, remove_missing=True)\n",
    "\n",
    "        gridded = np.full((len(gauges.index), len(new_y), len(new_x)), np.nan) # check dimensions correct way round\n",
    "\n",
    "        for t in range(len(gauges.index)):\n",
    "\n",
    "            vals = gauges.iloc[t].values\n",
    "            tot = np.nansum(vals)\n",
    "\n",
    "            if not np.isnan(tot):\n",
    "                if tot == 0:\n",
    "                    gridded[t] = 0\n",
    "                else:\n",
    "                    try:\n",
    "                        gridded[t] = np.flip(ok(vals).astype(float).reshape((len(new_y), len(new_x))), axis=0)\n",
    "                    except:\n",
    "                        pass    \n",
    "\n",
    "        full_rainfall[\"arrays\"] = gridded\n",
    "    \n",
    "else:\n",
    "    # INSUFFICIENT DATA FOR THE CODE TO RUN\n",
    "    logger.error(\"Insufficient data for the code to run. Model halted.\")\n",
    "    raise RuntimeError(\"Insufficient data for the code to run. Model halted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d71515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip to SHETRAN domain and save\n",
    "\n",
    "output_shetran_full = np.full(\n",
    "    (full_rainfall[\"arrays\"].shape[0], \n",
    "     new_y.shape[0], \n",
    "     new_x.shape[0]), np.nan)\n",
    "\n",
    "xs = full_rainfall[\"coords_x\"].values\n",
    "ys = np.flip(full_rainfall[\"coords_y\"].values)\n",
    "\n",
    "new_x_cond = np.array([min(abs(x - xs)) < 500 for x in new_x])\n",
    "new_y_cond = np.array([min(abs(y - ys)) < 500 for y in new_y])\n",
    "\n",
    "old_x_cond = np.array([min(abs(x - new_x)) < 500 for x in xs])\n",
    "old_y_cond = np.array([min(abs(y - new_y)) < 500 for y in ys])\n",
    "\n",
    "if not all(old_y_cond):\n",
    "    input1 = full_rainfall[\"arrays\"][:, old_y_cond, :]\n",
    "else:\n",
    "    input1 = full_rainfall[\"arrays\"]\n",
    "    \n",
    "if not all(old_x_cond):\n",
    "    input2 = input1[:, :, old_x_cond]\n",
    "else:\n",
    "    input2 = input1\n",
    "\n",
    "if all(new_x_cond) and all(new_y_cond):\n",
    "    output_shetran_full = input2\n",
    "elif all(new_x_cond):\n",
    "    output_shetran_full[:, new_y_cond, :] = input2\n",
    "else:\n",
    "    output_shetran_full[:, new_y_cond, :][:, :, new_x_cond] = input2\n",
    "\n",
    "output_shetran = pd.DataFrame(output_shetran_full[:, mask == 0])\n",
    "output_shetran.to_csv(join(output_shetran_path, \"Tyne_at_Newcastle_Precip.csv\"), index=False)\n",
    "\n",
    "# save timestamp info\n",
    "ts = pd.to_datetime(full_rainfall[\"timestamp\"])\n",
    "shetran_meta = pd.DataFrame([ts.min(), ts.max(), np.median(np.diff(ts))], index=[\"start_time\", \"end_time\", \"resolution\"], columns=[\"Value\"])\n",
    "shetran_meta.to_csv(join(output_shetran_path, \"time_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f30bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip data to HIPIMS domain and save\n",
    "\n",
    "# path to HIPIMS bbox file\n",
    "hipims_bbox_path = join(static_path, \"boundbox.shp\")\n",
    "bbox = gpd.read_file(hipims_bbox_path)\n",
    "\n",
    "# reproject bounding box for data\n",
    "bbox['geometry'] = bbox['geometry'].to_crs(epsg=27700)\n",
    "min_x, min_y, max_x, max_y = bbox.bounds.loc[0]\n",
    "\n",
    "if not np.isnan(hipims_method):\n",
    "    \n",
    "    # high-resolution data \n",
    "    try:\n",
    "        \"\"\"\n",
    "        Note: This does not work as high_res_rainfall dictionary \n",
    "        not defined as code does not existdo to maintence issues \n",
    "        of high resolution X-band radar data at Urban Observatory.\n",
    "        \"\"\"\n",
    "        \n",
    "        xs = high_res_rainfall[\"coords_x\"]\n",
    "        ys = high_res_rainfall[\"coords_y\"]\n",
    "        arrs = high_res_rainfall[\"arrays\"]\n",
    "        \n",
    "    except:\n",
    "        logger.error(\"High resolution rainfall does not exist.\")\n",
    "        xs = full_rainfall[\"coords_x\"]\n",
    "        ys = full_rainfall[\"coords_y\"]\n",
    "        arrs = full_rainfall[\"arrays\"]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # no high-resolution data\n",
    "    xs = full_rainfall[\"coords_x\"]\n",
    "    ys = full_rainfall[\"coords_y\"]\n",
    "    arrs = full_rainfall[\"arrays\"]\n",
    "    \n",
    "    \n",
    "x_cond = (xs >= min_x - 500) & (xs <= max_x + 500)\n",
    "y_cond = (ys >= min_y - 500) & (ys <= max_y + 500)\n",
    "clip_x = arrs[:, :, x_cond]\n",
    "clipped_rainfall = clip_x[:, y_cond[::-1], :]\n",
    "\n",
    "# Reformat to dataframe for input file\n",
    "data = clipped_rainfall.reshape(clipped_rainfall.shape[0], clipped_rainfall.shape[1] * clipped_rainfall.shape[2])\n",
    "clipped_rainfall_hipims = pd.DataFrame(data, index=full_rainfall[\"timestamp\"])\n",
    "\n",
    "# Save file\n",
    "clipped_rainfall_hipims.to_csv(join(output_hipims_path, \"rain_source.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa51329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished successfully\n",
    "logger.info(\"Model execution completed successfully\")\n",
    "pathlib.Path(os.path.join(output_path, PREPARE_DATA_SUCCESS_FILENAME)).touch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
